{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from pandas_gbq import to_gbq\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import warnings\n",
        "import json\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#expiry_time = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# build your token-URL once\n",
        "TOKEN_URL = (\n",
        "    f\"https://{os.environ['ATSCALE_URL']}\"\n",
        "    \"/auth/realms/atscale/protocol/openid-connect/token\"\n",
        ")\n",
        "\n",
        "def fetch_tokens(grant_type=\"password\", **extra_fields):\n",
        "    \"\"\"\n",
        "    grant_type: \"password\" or \"refresh_token\"\n",
        "    extra_fields: e.g. username, password, refresh_token\n",
        "    \"\"\"\n",
        "    cmd = [\n",
        "        \"curl\", \"--location\", TOKEN_URL,\n",
        "        \"--insecure\",\n",
        "        \"--header\", \"Content-Type: application/x-www-form-urlencoded\",\n",
        "        \"--data-urlencode\", f\"client_id=atscale-modeler\",\n",
        "        \"--data-urlencode\", f\"client_secret={os.environ['ATSCALE_CLIENT_SECRET']}\",\n",
        "        \"--data-urlencode\", f\"grant_type={grant_type}\"\n",
        "    ]\n",
        "    # append the extra form fields\n",
        "    for key, val in extra_fields.items():\n",
        "        cmd += [\"--data-urlencode\", f\"{key}={val}\"]\n",
        "    raw = subprocess.check_output(cmd)\n",
        "    return json.loads(raw)\n",
        "\n",
        "# 1) Initial fetch with user/pass\n",
        "token_payload = fetch_tokens(\n",
        "    grant_type=\"password\",\n",
        "    username=os.environ[\"ATSCALE_USER\"],\n",
        "    password=os.environ[\"ATSCALE_USER_PASSWORD\"]\n",
        ")\n",
        "\n",
        "# compute absolute expiry timestamp\n",
        "now_utc = datetime.datetime.now(datetime.timezone.utc)\n",
        "expires_in = token_payload[\"expires_in\"]  # seconds\n",
        "expiry_time = now_utc + datetime.timedelta(seconds=expires_in)\n",
        "\n",
        "access_token  = token_payload[\"access_token\"]\n",
        "refresh_token = token_payload[\"refresh_token\"]\n",
        "\n",
        "\n",
        "# --- later, before any API call, check & refresh if needed ---\n",
        "def ensure_token():\n",
        "    global access_token, refresh_token, expiry_time, token_payload\n",
        "\n",
        "    # use timezone-aware now()\n",
        "    now_utc = datetime.datetime.now(datetime.timezone.utc)\n",
        "\n",
        "    if now_utc >= expiry_time:\n",
        "        # time to refresh\n",
        "        token_payload = fetch_tokens(\n",
        "            grant_type=\"refresh_token\",\n",
        "            refresh_token=refresh_token\n",
        "        )\n",
        "        expires_in   = token_payload[\"expires_in\"]\n",
        "        expiry_time  = now_utc + datetime.timedelta(seconds=expires_in)\n",
        "        access_token = token_payload[\"access_token\"]\n",
        "        refresh_token= token_payload[\"refresh_token\"]\n",
        "        print(\"🔄 Refreshed access token\")\n",
        "\n",
        "    return access_token\n",
        "\n",
        "# Usage:\n",
        "# headers = {\"Authorization\": f\"Bearer {ensure_token()}\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1726060311747,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "PaCKgS2mwlPQ"
      },
      "outputs": [],
      "source": [
        "# === Your BigQuery setup ===\n",
        "project_id    = os.getenv(\"GBQ_PROJECT_ID\")\n",
        "dataset_id    = os.getenv(\"GBQ_DATASET_ID\")\n",
        "table_name_atscale_query_log = os.getenv(\"GBQ_TABLE_ID_ATSCALE_QUERY_LOG\")\n",
        "full_table_id = f\"{dataset_id}.{table_name_atscale_query_log}\"\n",
        "\n",
        "full_load_hours = 160 # Fallback to current time minus this many hours if query fails. Max limit from atscale <168"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the scope (can be more specific if needed)\n",
        "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
        "\n",
        "\n",
        "# Authenticate using the service account file\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    os.getenv(\"GBQ_SERVICE_ACCOUNT_FILE\"),\n",
        "    scopes=SCOPES\n",
        ")\n",
        "\n",
        "# Example: Initialize a client (e.g., Google Cloud Storage)\n",
        "client = bigquery.Client(credentials=credentials)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1913,
          "status": "ok",
          "timestamp": 1726060313659,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "OI1KZ0joxftU",
        "outputId": "57c173c6-eafe-427d-b83f-d53c4c69ee7e"
      },
      "outputs": [],
      "source": [
        "# Get the most recent startTime\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT startTime\n",
        "FROM `{full_table_id}`\n",
        "ORDER BY startTime DESC\n",
        "LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    query_job = client.query(query)\n",
        "    result = query_job.result()\n",
        "    # Grab the first row if available\n",
        "    row = next(iter(result), None)\n",
        "    if row and hasattr(row, 'startTime'):\n",
        "        last_load = row.startTime.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    else:\n",
        "        raise ValueError(\"No results returned for startTime\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to retrieve last load timestamp: %s\", e)\n",
        "    # Fallback: current time minus specified hours\n",
        "    fallback_time = datetime.datetime.now() - datetime.timedelta(hours=full_load_hours)\n",
        "    last_load = fallback_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "\n",
        "print(\"Last load timestamp:\", last_load)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Helper to flatten result + events + subqueries ===\n",
        "def extract_rows(result):\n",
        "    \"\"\"\n",
        "    For a single 'result' dict, this will produce one row per (event, subquery). \n",
        "    If an event has no 'subqueries' or an empty list, we still emit exactly one row\n",
        "    with the subquery-related columns set to None.\n",
        "    \"\"\"\n",
        "    # ‣ Collect all result-level fields first (timestamps still in ms)\n",
        "    base_info = {\n",
        "        \"status\":        result.get(\"status\"),\n",
        "        \"modelName\":     result.get(\"modelName\"),\n",
        "        \"catalogName\":   result.get(\"catalogName\"),\n",
        "        \"user\":          result.get(\"user\"),\n",
        "        \"queryId\":       result.get(\"queryId\"),\n",
        "        \"startTime\":     result.get(\"startTime\"),   # ms\n",
        "        \"duration\":      result.get(\"duration\"),    # numeric\n",
        "        \"attributes\":    \", \".join(result.get(\"attributes\", [])),\n",
        "        \"measures\":      \", \".join(result.get(\"measures\", [])),\n",
        "        \"aggregates\":    \", \".join(result.get(\"aggregates\", [])),\n",
        "        \"optimization\":  \", \".join(result.get(\"optimization\", [])),\n",
        "        \"failedMessage\": result.get(\"failedMessage\", None),\n",
        "    }\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    #  Loop over each event in result[\"events\"]\n",
        "    for event in result.get(\"events\", []):\n",
        "        # Capture event-level fields (still in ms)\n",
        "        event_info = {\n",
        "            \"event_name\":      event.get(\"name\"),\n",
        "            \"event_startTime\": event.get(\"startTime\"),   # ms\n",
        "            \"event_duration\":  event.get(\"duration\"),    # numeric\n",
        "        }\n",
        "\n",
        "        # Now check subqueries for this event\n",
        "        subqueries = event.get(\"subqueries\", [])\n",
        "\n",
        "        if subqueries and isinstance(subqueries, list):\n",
        "            # If there are one or more subqueries, loop through them\n",
        "            for subq in subqueries:\n",
        "                # Start with the base_info + event_info\n",
        "                row = {}\n",
        "                row.update(base_info)\n",
        "                row.update(event_info)\n",
        "\n",
        "                # Flatten every key in the subquery under \"subquery_<key>\"\n",
        "                # For your JSON, subq typically has: name, startTime, duration, subqueryId\n",
        "                for key, val in subq.items():\n",
        "                    col_name = f\"subquery_{key}\"\n",
        "                    row[col_name] = val\n",
        "\n",
        "                rows.append(row)\n",
        "        else:\n",
        "            # No subqueries for this event → emit a single row with subquery_* = None\n",
        "            row = {}\n",
        "            row.update(base_info)\n",
        "            row.update(event_info)\n",
        "\n",
        "            # We know the JSON's subquery objects have keys: name, startTime, duration, subqueryId\n",
        "            # So we explicitly add those columns as None\n",
        "            row[\"subquery_name\"]       = None\n",
        "            row[\"subquery_startTime\"]  = None\n",
        "            row[\"subquery_duration\"]   = None\n",
        "            row[\"subquery_subqueryId\"] = None\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === API Setup ===\n",
        "url = f\"https://{os.environ['ATSCALE_URL']}/api/queries?showCanaries=false&startDate={last_load}\"  \n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer \" + ensure_token(),  \n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "page = 1\n",
        "page_size = 20  \n",
        "has_next_page = True\n",
        "all_rows = []\n",
        "\n",
        "# === Pagination loop ===\n",
        "while has_next_page:\n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"pageSize\": page_size\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers, verify=False, params=params)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract results and append rows\n",
        "    for result in data.get(\"results\", []):\n",
        "        all_rows.extend(extract_rows(result))\n",
        "\n",
        "    # Determine whether to continue\n",
        "    has_next_page = data.get(\"hasNextPage\", False)\n",
        "    page += 1\n",
        "\n",
        "# === Create final DataFrame ===\n",
        "combined_df = pd.DataFrame(all_rows)\n",
        "\n",
        "# Example: show results\n",
        "combined_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Do some cleansing ===\n",
        "\n",
        "def clean_dataframe_for_bigquery(df):\n",
        "    cleaned_df = df.copy()\n",
        "    \n",
        "    # (A1) Suppose your timestamp columns are:\n",
        "    datetime_cols = [\"startTime\", \"event_startTime\", \"subquery_startTime\"]\n",
        "\n",
        "    # (A2) And your purely numeric columns are (durations, etc.):\n",
        "    numeric_cols = [\"duration\", \"event_duration\", \"subquery_duration\"]\n",
        "\n",
        "    for col in cleaned_df.columns:\n",
        "        if col in datetime_cols:\n",
        "            # Ensure datetime is clean\n",
        "\n",
        "            cleaned_df[col] = pd.to_datetime(cleaned_df[col], unit='ms' , errors=\"coerce\").values.astype(\"datetime64[us]\")\n",
        "\n",
        "            continue\n",
        "\n",
        "        # If any of these still show up as object, explicitly cast them now:\n",
        "        if col in numeric_cols:\n",
        "            cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors=\"coerce\")\n",
        "            continue\n",
        "\n",
        "        # If values are dict or list, convert to JSON strings\n",
        "        if cleaned_df[col].apply(lambda x: isinstance(x, (dict, list))).any():\n",
        "            cleaned_df[col] = cleaned_df[col].apply(json.dumps)\n",
        "\n",
        "        # Replace None/NaN with empty string, cast to string\n",
        "        #cleaned_df[col] = cleaned_df[col].apply(lambda x: \"\" if x is None or pd.isna(x) else str(x))\n",
        "        cleaned_df[col] = cleaned_df[col].fillna(\"\").astype(str)\n",
        " \n",
        "        # Finally cast to a PyArrow‐safe string dtype\n",
        "        cleaned_df[col] = cleaned_df[col].astype(str)\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Use this before uploading\n",
        "cleaned_df = clean_dataframe_for_bigquery(combined_df)\n",
        "\n",
        "cleaned_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === Append the DataFrame ===\n",
        "to_gbq(\n",
        "    cleaned_df,\n",
        "    destination_table=full_table_id,\n",
        "    project_id=project_id,\n",
        "    credentials=credentials,\n",
        "    location=os.getenv(\"GBQ_LOCATION\"),\n",
        "    if_exists=\"append\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "name": "GetAtScaleQueryData",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
